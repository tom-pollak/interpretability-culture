{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp.all import *\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch as t\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "sae_pretrained_path = Path(__file__).parents[1] / \"culture-gpt-0-sae\"\n",
    "\n",
    "model: HookedTransformer\n",
    "model = load_hooked(0).eval().to(device)  # type: ignore\n",
    "\n",
    "\n",
    "def load_sae(path: str) -> SAE:\n",
    "    from sae_lens import SAEConfig\n",
    "    from sae_lens.toolkit.pretrained_sae_loaders import read_sae_from_disk\n",
    "    from sae_lens.config import DTYPE_MAP\n",
    "\n",
    "    weight_path = hf_hub_download(SAE_REPO_ID, path + \"/sae_weights.safetensors\")\n",
    "    cfg_path = hf_hub_download(SAE_REPO_ID, path + \"/cfg.json\")\n",
    "    with open(cfg_path, \"r\") as f:\n",
    "        cfg_dict = json.load(f)\n",
    "    cfg_dict, state_dict = read_sae_from_disk(\n",
    "        cfg_dict=cfg_dict,\n",
    "        weight_path=weight_path,\n",
    "        device=\"cpu\",\n",
    "        dtype=DTYPE_MAP[cfg_dict[\"dtype\"]],\n",
    "    )\n",
    "    print(cfg_dict)\n",
    "    sae_cfg = SAEConfig.from_dict(cfg_dict)\n",
    "    sae_cfg.device = str(device)\n",
    "    sae = SAE(sae_cfg)\n",
    "    sae.load_state_dict(state_dict)\n",
    "    sae.eval()\n",
    "    return sae\n",
    "\n",
    "\n",
    "sae = load_sae(\"gpt-0/blocks_8_mlp_out\")\n",
    "\n",
    "dataset = load_dataset(\"tommyp111/culture-puzzles-1M-partitioned\")\n",
    "assert isinstance(dataset, DatasetDict)\n",
    "dataset.set_format(\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit of a util for later, The first and second grid are a one-shot example, so we should not include these in our eval / analysis. Let's create slices that extract the final and second final grid.\n",
    "\n",
    "(not including the special tokens A f(A) etc. These are easy).\n",
    "\n",
    "Another gotcha: these should be from a \"stripped\" batch, i.e. a batch without it's final token. I've found this edge case to be rather strange, as\n",
    "\n",
    "\n",
    "I've found this edge case to be rather strange, and HookedTransformer complains about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grid_slice = slice(-100, None)\n",
    "second_final_grid_slice = slice(-201, -101)\n",
    "\n",
    "batch = dataset[\"contact\"][\"input_ids\"][:5].to(device)\n",
    "batch_stripped = batch[:, :]\n",
    "\n",
    "a = batch_stripped[:1].clone()\n",
    "print(repr_grid(a[0]))\n",
    "print(\"\\n████████████████████████████████████████████████████████████████████████████████\\n\")\n",
    "a[:, second_final_grid_slice] = 4\n",
    "a[:, final_grid_slice] = 5\n",
    "print(repr_grid(a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained on layer 8 somewhat arbitarily, let's see if we can find some interesting behaviour attributing to this MLP layer.\n",
    "\n",
    "Looking at the loss can be less informative, because for most of the tokens it is extremely staight-forward (as shown in the first notebook).\n",
    "\n",
    "Instead let's look at the accuracy -- if the model get's the entire grid correct. The models all have a accuracy of ~95% so this can be a much more useful measure than it would be in a text GPT (where it is pretty much impossible to predict the next passage perfectly).\n",
    "\n",
    "I'll use the partitioned dataset so we can look at each task individually -- I think this is quite principled, since each example in the training dataset only included a single task. It makes sense for ability / features to be task specific.\n",
    "\n",
    "(The culture may be a mix of all tasks, generated by the models themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_abl_hook(activation, hook):\n",
    "    return t.zeros_like(activation)\n",
    "\n",
    "\n",
    "def ablate_mlp_single_task(batch, layer):\n",
    "    # Batch: we want to look at predictions for only the final grid\n",
    "    final_grid_slice = slice(-100, None)\n",
    "\n",
    "    # Logits: the logit predictions are shape (405,) which predict the token *after* the current.\n",
    "    # aka the BOS token is not predicted in the logits, but the token *after* the final grid\n",
    "    # is. Therefore we need to strip this after final grid logit off.\n",
    "    final_grid_strip = slice(-101, -1)\n",
    "\n",
    "    with t.inference_mode():\n",
    "        logits_orig, loss_orig = model(batch, return_type=\"both\")\n",
    "        logits_orig = logits_orig[:, final_grid_strip] # use strip on the logits\n",
    "\n",
    "        logits_abl, loss_abl = model.run_with_hooks(\n",
    "            batch,\n",
    "            return_type=\"both\",\n",
    "            fwd_hooks=[(f\"blocks.{layer}.hook_mlp_out\", zero_abl_hook)],\n",
    "        )\n",
    "        logits_abl = logits_abl[:, final_grid_strip]\n",
    "\n",
    "    # argmax => temp 0\n",
    "    orig_correct = t.all(batch[:, final_grid_slice] == logits_orig.argmax(-1), dim=1)\n",
    "    abl_same = t.all(logits_orig.argmax(-1) == logits_abl.argmax(-1), dim=1)\n",
    "    wrong_puzzles = (~orig_correct).argwhere()[:, 0].tolist()\n",
    "    abl_diff_puzzles = (~abl_same).argwhere()[:, 0].tolist()\n",
    "\n",
    "    wrong_and_different_ablate = list(\n",
    "        set(wrong_puzzles).intersection(set(abl_diff_puzzles))\n",
    "    )\n",
    "    wrong_because_ablate = list(set(abl_diff_puzzles).difference(set(wrong_puzzles)))\n",
    "\n",
    "    print(\"wrong puzzles:\", wrong_puzzles)\n",
    "    print(\"different w/ ablation:\", abl_diff_puzzles)\n",
    "    print(\"different abaltation & wrong:\", wrong_and_different_ablate)\n",
    "    print(\"wrong because of ablation:\", wrong_because_ablate)\n",
    "    print()\n",
    "    return wrong_because_ablate\n",
    "\n",
    "\n",
    "n = 100\n",
    "layer = 8\n",
    "for task, data in dataset.items():\n",
    "    print(\"task:\", task)\n",
    "    batch = data[:n][\"input_ids\"].to(device)\n",
    "    ablate_mlp_single_task(batch, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Ablating layer 8 seems to primarily only impact only the contact task, (not including culture, mix of all tasks)\n",
    "Let's have a look at the quizzes the model got wrong only with ablation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "layer = 8\n",
    "batch = dataset[\"contact\"][:n][\"input_ids\"].to(device)\n",
    "wrong_because_ablate = ablate_mlp_single_task(batch, layer)\n",
    "\n",
    "for i in range(min(5, len(wrong_because_ablate))):\n",
    "    model.add_hook(\"blocks.8.hook_mlp_out\", zero_abl_hook)  # type: ignore\n",
    "    generate_and_print(model, batch[wrong_because_ablate[i]], temperature=0.0, verbose=False)\n",
    "    print(\"#\" * 100)\n",
    "    model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the examples seem to fall victim to the same problem!\n",
    "\n",
    "# Problem\n",
    "\n",
    "Given the one-shot example, two of the squares \"contact\", and one encircles the other square with their color.\n",
    "\n",
    "- There are often other squares that do not contact, and are therefore not encircled.\n",
    "- You must use the one-shot example to deem which of the squares is encircled, and which is the encircler.\n",
    "\n",
    "When this layer is ablated, it seems to lose the ability to lookback and see\n",
    "- which square should be encircled\n",
    "- which squares should be ignored\n",
    "\n",
    "\n",
    "Instead it encircles every square with the encircler! (but keeps the correct color)\n",
    "\n",
    "Using train_sae.py, I trained a model on layer 8. This uses all data in tommyp111/culture-puzzles-1M, not just this particular task. Let's see if applying the SAE weights can resolve this problem for these tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    _, T, _ = activation.shape\n",
    "    return sae_out[None, :T, :]\n",
    "\n",
    "model.reset_hooks()\n",
    "with t.inference_mode():\n",
    "    _, cache = model.run_with_cache(batch[wrong_because_ablate, :-1])\n",
    "    h = cache[sae.cfg.hook_name].clone()\n",
    "    del cache\n",
    "    feature_acts = sae.encode(h)\n",
    "    del h\n",
    "    sae_out = sae.decode(feature_acts)\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "tasks_correct = []\n",
    "for i in range(min(5, len(wrong_because_ablate))):\n",
    "    model.add_hook(sae.cfg.hook_name, partial(reconstr_hook, sae_out=sae_out[i]))  # type: ignore\n",
    "    correct, _ = generate_and_print(\n",
    "        model, batch[wrong_because_ablate[i]], temperature=0.0\n",
    "    )\n",
    "    tasks_correct.append(correct)\n",
    "    model.reset_hooks()\n",
    "\n",
    "print(\"All tasks correct:\", all(tasks_correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay it does! The SAE fixes the behaviour on all quizzes that the model failed on with zero ablation, this could be a good indicator that my SAE is working correctly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's take a look at the loss w.r.t normal, zero and reconstruction for the all tasks and the contact task specifically.\n",
    "\n",
    "- Like before, I'm going to look at the loss at only the final grid where the square is different to the square from the previous grid -- the \"transform\" squares\n",
    "\n",
    "- Investiage which W_dec neurons fire\n",
    "\n",
    "- Find the mlp in the other models that cause this same behaviour -- is this univeral neurons?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
